{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4RiiXJ1GWrKC"
      },
      "outputs": [],
      "source": [
        "#@title Install required packages and Auto - Restart Session\n",
        "\n",
        "!pip install accelerate==0.32.1\n",
        "# !pip install --upgrade transformers\n",
        "!pip install transformers==4.44.0\n",
        "!pip install bitsandbytes==0.43.3\n",
        "!pip install sentencepiece==0.1.99\n",
        "!pip install protobuf==3.20.3\n",
        "!pip install flash-attn==2.6.3\n",
        "!pip install gradio==4.36.1\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "import time\n",
        "time.sleep(5)\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title <-- Play Audio to active the colab Notebook { display-mode: \"form\" }\n",
        "\n",
        "%%html\n",
        "<b>Press play on the music player to keep the tab alive, then run the cell below</b><br/>\n",
        "<audio src=\"https://raw.githubusercontent.com/KoboldAI/KoboldAI-Client/main/colab/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "bkO1zsALhdTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hXJoWtCXlkR",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Import & Download Hermes-3-Llama-3.1-8B Model\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM\n",
        "import bitsandbytes, flash_attn\n",
        "import gc\n",
        "import time\n",
        "import subprocess\n",
        "\n",
        "def is_gpu_memory_over_limit(limit_gb=14.5):\n",
        "    # Run nvidia-smi and capture the output\n",
        "    result = subprocess.run(['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader'],\n",
        "                            stdout=subprocess.PIPE, text=True)\n",
        "\n",
        "    # Split the result into lines (for each GPU if there are multiple)\n",
        "    memory_used_mb_list = result.stdout.strip().splitlines()\n",
        "\n",
        "    # Convert memory used from MB to GB and check each GPU's memory usage\n",
        "    for i, memory_used_mb in enumerate(memory_used_mb_list):\n",
        "        memory_used_gb = int(memory_used_mb) / 1024.0\n",
        "        print(f\"GPU {i}: Current memory allocated: {memory_used_gb:.2f} GB\")\n",
        "        if memory_used_gb > limit_gb:\n",
        "            print(f\"GPU {i} memory usage exceeds {limit_gb} GB.\")\n",
        "            return True\n",
        "\n",
        "    print(\"GPU memory usage is within safe limits.\")\n",
        "    return False\n",
        "\n",
        "\n",
        "tokenizer=None\n",
        "model=None\n",
        "def load_Llama():\n",
        "  global tokenizer,model\n",
        "  try:\n",
        "    if tokenizer is not None:\n",
        "      del tokenizer\n",
        "      tokenizer=None\n",
        "    if model is not None:\n",
        "      del model\n",
        "      model=None\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"Free GPU memeory\")\n",
        "    time.sleep(2)\n",
        "  except:\n",
        "    pass\n",
        "  tokenizer = AutoTokenizer.from_pretrained('NousResearch/Hermes-3-Llama-3.1-8B', trust_remote_code=True)\n",
        "  model = LlamaForCausalLM.from_pretrained(\n",
        "      \"NousResearch/Hermes-3-Llama-3.1-8B\",\n",
        "      torch_dtype=torch.float16,\n",
        "      device_map=\"auto\",\n",
        "      load_in_8bit=False,\n",
        "      load_in_4bit=True,\n",
        "      # use_flash_attention_2=True\n",
        "  )\n",
        "  return tokenizer,model\n",
        "\n",
        "import re\n",
        "\n",
        "def clean_response(text):\n",
        "    # Remove newlines\n",
        "    text = text.replace('\\n', ' ')\n",
        "    # Remove words inside * *\n",
        "    cleaned_text = re.sub(r'\\*.*?\\*', '', text)\n",
        "    # Remove extra spaces\n",
        "    cleaned_text = ' '.join(cleaned_text.split())\n",
        "    return cleaned_text.strip()\n",
        "\n",
        "def Llama_Chat(system_role,user_msg):\n",
        "  global tokenizer,model\n",
        "  prompts = [\n",
        "              f\"\"\"<|im_start|>system\n",
        "          {system_role}<|im_end|>\n",
        "          <|im_start|>user\n",
        "          {user_msg}<|im_end|>\n",
        "          <|im_start|>assistant\"\"\",\n",
        "              ]\n",
        "  ans=[]\n",
        "  for chat in prompts:\n",
        "      # print(chat)\n",
        "      input_ids = tokenizer(chat, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "      generated_ids = model.generate(input_ids, max_new_tokens=750, temperature=0.8, repetition_penalty=1.1, do_sample=True, eos_token_id=tokenizer.eos_token_id)\n",
        "      response = tokenizer.decode(generated_ids[0][input_ids.shape[-1]:], skip_special_tokens=True, clean_up_tokenization_space=True)\n",
        "      # print(f\"Response: {response}\")\n",
        "      ans.append(response)\n",
        "  reply=clean_response(ans[-1])\n",
        "  return reply\n",
        "\n",
        "tokenizer,model=load_Llama()\n",
        "def run_llama(system_role,user_msg):\n",
        "  global tokenizer,model\n",
        "  if is_gpu_memory_over_limit():\n",
        "    tokenizer,model=load_Llama()\n",
        "  llama_reply=Llama_Chat(system_role,user_msg)\n",
        "  return llama_reply\n",
        "\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "6kKqovDDZa8w",
        "outputId": "c889f215-7b48-447e-95b3-cf61a43cc609",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I am an AI language model designed to assist and engage in conversation on a wide range of topics. I aim to provide accurate, relevant information in a friendly manner. How can I help you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "#@title Test Hermes-3-Llama-3.1-8B\n",
        "system_role= \"You are a helpful Assistant, friendly and fun, providing users with short and concise answers to their requests.\"  # @param {type: \"string\"}\n",
        "user_msg = \"Who are you?\"  # @param {type: \"string\"}\n",
        "llama_reply=run_llama(system_role,user_msg)\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "llama_reply"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "I4RM1ThImO9n"
      },
      "outputs": [],
      "source": [
        "#@title Gradio API Interface\n",
        "username = 'admin'  # @param {type: \"string\"}\n",
        "password = 'admin'  # @param {type: \"string\"}\n",
        "Debug = True  # @param {type: \"boolean\"}\n",
        "\n",
        "import gradio as gr\n",
        "gradio_examples = [[\"You are a helpful Assistant, friendly and fun, providing users with short and concise answers to their requests\",\"Who are you?\"]]\n",
        "gradio_input=[gr.Textbox(label=\"System Role\"),gr.Textbox(label=\"User Message\")]\n",
        "gradio_output=[gr.Textbox(label=\"Hermes-3-Llama-3.1-8B Response\")]\n",
        "gradio_interface = gr.Interface(fn=run_llama, inputs=gradio_input,outputs=gradio_output , title=\"Hermes-3-Llama-3.1-8B\",examples=gradio_examples)\n",
        "gradio_interface.queue().launch(share=True,debug=Debug,auth=(username, password))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DrjG3FaUjTO9"
      },
      "outputs": [],
      "source": [
        "#@title Gradio Chat Interface\n",
        "# import time\n",
        "# import gradio as gr\n",
        "\n",
        "\n",
        "# def slow_echo(user_msg, history):\n",
        "#     system_role= \"You are a helpful Assistant, friendly and fun, providing users with short and concise answers to their requests.\"  # @param {type: \"string\"}\n",
        "#     message=Llama_Chat(system_role,user_msg)\n",
        "#     for i in range(len(message)):\n",
        "#         time.sleep(0.05)\n",
        "#         yield \"\" + message[: i + 1]\n",
        "\n",
        "\n",
        "# demo = gr.ChatInterface(slow_echo)\n",
        "# demo.launch(share=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}